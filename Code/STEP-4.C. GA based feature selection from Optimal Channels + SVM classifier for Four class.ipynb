{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LEUTjPds57fW"},"outputs":[],"source":["import sys\n","import warnings\n","if not sys.warnoptions:\n","  warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hR-yxs2d6d4Q"},"outputs":[],"source":["! pip install deap\n","! pip install scoop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UYpJjc9j6FKL"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","import sklearn.model_selection as model_selection\n","import pickle, scipy, csv, statistics, math, warnings, joblib, random, numpy\n","from sklearn import metrics, svm, datasets\n","from math import log,e, floor\n","from time import time\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.model_selection import cross_val_score,cross_val_predict,GridSearchCV, train_test_split, StratifiedKFold, KFold, cross_validate, learning_curve\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n","from sklearn.metrics import plot_confusion_matrix, mean_absolute_error, accuracy_score, r2_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n","from sklearn import metrics, preprocessing, svm\n","from sklearn.linear_model import LogisticRegression\n","from deap import creator, base, tools, algorithms\n","from scoop import futures\n","from sklearn.utils import shuffle\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30773,"status":"ok","timestamp":1640500826375,"user":{"displayName":"SHYAM MARJIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgXWgt6yPRlb1Vc2PDp-7CmKEzAlS0XLO2cxEV=s64","userId":"10874093040693940713"},"user_tz":-330},"id":"xpDM7M7a9poR","outputId":"faa24a5f-dbee-4bdd-ce48-331808c09440"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3ULU-z8dNFX"},"outputs":[],"source":["def emotion_label(labels, class_label):\n","\tem_labels = []\n","\tif(class_label == \"valence\"):\n","\t\tfor i in range(0, labels.shape[0]):\n","\t\t\tif (labels[i][0]>5): # high valence\n","\t\t\t\tem_labels.append(1)\n","\t\t\telse: # low valence\n","\t\t\t\tem_labels.append(0)\n","\t\treturn em_labels\n","\telif(class_label == \"arousal\"):\n","\t\tfor i in range(0, labels.shape[0]):\n","\t\t\tif (labels[i][1]>5): # high arousal\n","\t\t\t\tem_labels.append(1)\n","\t\t\telse: # low arousal\n","\t\t\t\tem_labels.append(0)\n","\t\treturn em_labels\n","\telif(class_label == \"all\"):\n","\t\tfor i in range(0, labels.shape[0]):\n","\t\t\tif (labels[i][0]>5): # high valence\n","\t\t\t\tif(labels[i][1]>5): # high arousal\n","\t\t\t\t\tem_labels.append(1) # HVHA\n","\t\t\t\telse:\n","\t\t\t\t\tem_labels.append(0) # HVLA\n","\t\t\telse: # low valence\n","\t\t\t\tif(labels[i][1]>5): # high arousal\n","\t\t\t\t\tem_labels.append(2) # LVHA\n","\t\t\t\telse: # low arousal\n","\t\t\t\t\tem_labels.append(3) # LVLA\n","\t\treturn em_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cS0m53fZ7LhK"},"outputs":[],"source":["def kfold(allFeatures, allClasses, index):\n","    m = int(allClasses.shape[0])\n","    s = int(m/10)\n","    train_index = np.ones(m-s+1).astype(int)\n","    test_index  = np.ones(s).astype(int)\n","    for i in range(s*index,s*(index+1)):\n","        test_index[i-s*index] = i\n","    for i in range(0,s*index):\n","        train_index[i] = i\n","    for i in range(s + s*index,m):\n","        train_index[i-s] = i\n","    X_train = allFeatures.iloc[train_index]\n","    X_test = allFeatures.iloc[test_index]\n","    y_train = allClasses[train_index]\n","    y_test = allClasses[test_index]\n","    # normalize the xtrain data only with class labels\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOgm0TEtANY3"},"outputs":[],"source":["# Feature subset fitness function\n","def getFitness(individual, X_train, X_test, y_train, y_test):\n","    \"\"\"\n","    parse our feature columns that we don't use\n","    apply one hot encoding to the features.\n","    \"\"\"\n","    cols = [index for index in range(len(individual)) if individual[index] == 0]\n","    X_trainParsed = X_train.drop(X_train.columns[cols], axis=1)\n","    X_trainOhFeatures = pd.get_dummies(X_trainParsed)\n","    X_testParsed = X_test.drop(X_test.columns[cols], axis=1)\n","    X_testOhFeatures = pd.get_dummies(X_testParsed)\n","\n","    # Remove any columns that aren't in both the training and test sets\n","    sharedFeatures = set(X_trainOhFeatures.columns) & set(X_testOhFeatures.columns)\n","    removeFromTrain = set(X_trainOhFeatures.columns) - sharedFeatures\n","    removeFromTest = set(X_testOhFeatures.columns) - sharedFeatures\n","    X_trainOhFeatures = X_trainOhFeatures.drop(list(removeFromTrain), axis=1)\n","    X_testOhFeatures = X_testOhFeatures.drop(list(removeFromTest), axis=1)\n","\n","    # Apply logistic regression on the data, and calculate accuracy\n","    clf = svm.SVC(kernel='poly').fit(X_trainOhFeatures, y_train)\n","    predictions = clf.predict(X_testOhFeatures)\n","    accuracy = accuracy_score(y_test, predictions)*100\n","    return (accuracy,)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvZAc8yv8UMt"},"outputs":[],"source":["def getFitness_all(individual, X_train, X_test, y_train, y_test):\n","    \"\"\"\n","    Feature subset fitness function\n","    Parse our feature columns that we don't use\n","    Apply one hot encoding to the features.\n","    \"\"\"\n","    cols = [index for index in range(len(individual)) if individual[index] == 0]\n","    #print(cols)\n","    #print(cols.shape)\n","    X_trainParsed = X_train.drop(X_train.columns[cols], axis=1)\n","    X_trainOhFeatures = pd.get_dummies(X_trainParsed)\n","    X_testParsed = X_test.drop(X_test.columns[cols], axis=1)\n","    X_testOhFeatures = pd.get_dummies(X_testParsed)\n","\n","    # Remove any columns that aren't in both the training and test sets\n","    sharedFeatures = set(X_trainOhFeatures.columns) & set(X_testOhFeatures.columns)\n","    removeFromTrain = set(X_trainOhFeatures.columns) - sharedFeatures\n","    removeFromTest = set(X_testOhFeatures.columns) - sharedFeatures\n","    X_trainOhFeatures = X_trainOhFeatures.drop(list(removeFromTrain), axis=1)\n","    X_testOhFeatures = X_testOhFeatures.drop(list(removeFromTest), axis=1)\n","\n","    # Apply logistic regression on the data, and calculate accuracy\n","    clf = svm.SVC(kernel='poly').fit(X_trainOhFeatures, y_train)\n","    predictions = clf.predict(X_testOhFeatures)\n","    accuracy = accuracy_score(y_test, predictions)*100\n","    print(\"Accuracy Score: \", accuracy)\n","    try:\n","      print('Precision Score : ', precision_score(y_test, predictions)*100)\n","      print('Recall Score : ', recall_score(y_test, predictions)*100)\n","      print('F1 Score : ', f1_score(y_test, predictions)*100)\n","      print('Confusion Matrix : \\n' + str(confusion_matrix(y_test, predictions)))\n","    except:\n","      pass\n","    return (accuracy, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtVNGpokANb_"},"outputs":[],"source":["def getHof(toolbox):\n","    # Initialize variables to use eaSimple\n","    numPop = 100\n","    numGen = 50\n","    pop = toolbox.population(n=numPop)\n","    hof = tools.HallOfFame(numPop * numGen)\n","    stats = tools.Statistics(lambda ind: ind.fitness.values)\n","    stats.register(\"avg\", numpy.mean)\n","    stats.register(\"std\", numpy.std)\n","    stats.register(\"min\", numpy.min)\n","    stats.register(\"max\", numpy.max)\n","\n","    # Launch genetic algorithm\n","    # change the crossover and mutation probability\n","    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.65, mutpb=0.001, ngen=numGen, stats=stats, halloffame=hof, verbose=False)\n","    # Return the hall of fame\n","    return hof,log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VD9A156cANen"},"outputs":[],"source":["def getMetrics(hof, X_train, X_test, y_train, y_test):\n","    # Get list of percentiles in the hall of fame\n","    percentileList = [i / (len(hof) - 1) for i in range(len(hof))]\n","    # Gather fitness data from each percentile\n","    testAccuracyList = []\n","    validationAccuracyList = []\n","    individualList = []\n","    for individual in hof:\n","        testAccuracy = individual.fitness.values\n","        validationAccuracy = getFitness(individual, X_train, X_test, y_train, y_test)\n","        testAccuracyList.append(testAccuracy[0])\n","        validationAccuracyList.append(validationAccuracy[0])\n","        individualList.append(individual)\n","    #testAccuracyList.reverse()\n","    #validationAccuracyList.reverse()\n","    return testAccuracyList, validationAccuracyList, individualList, percentileList"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmWt9S-B6gAm"},"outputs":[],"source":["def drive(subject_name, channel_name):\n","  dfData = pd.read_csv('/content/drive/MyDrive/Sequential methods for channel selection/our code/New/' + subject_name + '_FourClass.csv')\n","  allFeatures = dfData\n","  names = allFeatures.columns\n","  scaler = MinMaxScaler()\n","  allFeatures = scaler.fit_transform(allFeatures)\n","  allFeatures = pd.DataFrame(allFeatures, columns=names)\n","  allFeatures = allFeatures.loc[:, allFeatures.apply(pd.Series.nunique) != 1]\n","  link = \"/content/drive/MyDrive/Deap/\" + subject_name + \".dat\"\n","  with open(link, 'rb') as f:\n","    raw_data = pickle.load(f, encoding = 'latin1')\n","  labels = raw_data['labels']\n","  em_labels = emotion_label(labels, 'all') # get the emotion labels\n","  allClasses = np.array(em_labels)\n","  allFeatures, allClasses = shuffle(allFeatures, allClasses, random_state = 40)\n","  return allFeatures, allClasses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK82L6_S8Jgn"},"outputs":[],"source":["subject_names = [\"s01\", \"s02\", \"s03\", \"s04\", \"s05\", \"s06\", \"s07\", \"s08\", \"s09\", \"s10\", \"s11\", \"s12\",\n","                 \"s13\", \"s14\", \"s15\", \"s16\", \"s17\", \"s18\", \"s19\", \"s20\", \"s21\",\n","                \"s22\", \"s23\", \"s24\", \"s25\", \"s26\", \"s27\", \"s28\", \"s29\", \"s30\", \"s31\", \"s32\"]\n","subject_all_channels = [['P7', 'P8', 'Fz', 'CP6', 'PO3', 'FC5', 'FC1'], ['AF3', 'CP1', 'F7', 'O1', 'AF4', 'FC6'],\n","                        ['O2', 'Oz', 'O1'], ['P8', 'CP2', 'Cz'], ['Fz', 'O2', 'PO4', 'FC1', 'F7', 'Oz', 'CP6', 'Fp2', 'FC6', 'C4', 'Pz', 'CP5', 'O1', 'T8'],\n","                        ['CP1', 'T7', 'FC6', 'AF3'], ['PO4', 'Pz', 'Fp2', 'Cz', 'P8', 'P7', 'O1', 'FC2', 'CP2'], ['Fp1', 'CP6', 'O2', 'C4', 'T8', 'AF3', 'PO3'],\n","                        ['T7', 'AF4', 'T8', 'P8', 'Cz'], ['T7', 'P3', 'FC1', 'CP1', 'Cz', 'PO3', 'CP2', 'O1'], ['C3', 'P7', 'FC2', 'F4', 'CP2', 'P3', 'F8', 'FC6', 'F7', 'FC5', 'FC1', 'F3', 'CP5', 'CP1'],\n","                        ['Cz', 'P3', 'F3', 'AF3'], ['Cz', 'AF3', 'Pz', 'Fp2', 'Oz', 'P4', 'Fz', 'FC2', 'AF4', 'P8', 'F8', 'P7', 'CP1'], ['FC2', 'PO3', 'P7'],\n","                        ['Pz', 'FC6', 'P4', 'Fp1', 'PO3', 'AF3'], ['O2', 'FC1', 'T8'], ['CP1', 'CP6', 'T8', 'AF4', 'PO4', 'O2', 'Pz', 'O1', 'F4', 'C4'],\n","                        ['FC6', 'AF4', 'Pz', 'F8', 'O1'], ['P8', 'CP6', 'T8', 'F4'], ['AF4', 'Fz', 'PO3', 'FC2', 'CP5', 'F8', 'P4', 'T7', 'F7', 'Fp2', 'FC5', 'PO4', 'C3'],\n","                        ['P7', 'T8', 'P4', 'Fz', 'F8', 'Fp2'], ['O2', 'C3', 'P4', 'PO4', 'F3', 'PO3', 'Fz', 'Fp2', 'AF4', 'T8', 'Fp1', 'CP1'],\n","                        ['T7', 'Fp1', 'CP5', 'P8', 'F7', 'F8', 'AF4', 'FC2'], ['Pz', 'P8', 'CP2', 'O1', 'P3', 'CP1', 'Cz', 'C3', 'FC2', 'T7', 'Oz', 'P7', 'AF4', 'Fp2', 'FC6', 'F3', 'P4', 'Fp1', 'C4', 'AF3', 'CP6'],\n","                        ['FC2', 'FC5', 'F4'], ['P3', 'FC5'], ['Oz', 'T8', 'AF4', 'FC5', 'CP6', 'FC6', 'CP5', 'O2', 'CP1', 'F7', 'C4', 'F3', 'CP2', 'C3', 'P4', 'F8', 'T7', 'Cz', 'Fz'],\n","                        ['CP2', 'C3', 'Oz', 'FC1', 'Pz', 'O2', 'PO4', 'P4', 'Fz', 'P8', 'CP5', 'PO3', 'Fp1', 'FC6', 'AF4', 'Fp2', 'C4', 'CP6'],\n","                        ['AF3', 'CP6', 'O2', 'F8', 'O1', 'CP1', 'CP5'], ['Oz', 'PO4'], ['C4', 'F4'], ['CP6', 'CP2', 'CP5', 'O1', 'PO4', 'Fz', 'AF4', 'Cz', 'Oz']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9oFiOjcAZot"},"outputs":[],"source":["def main_code(allFeatures, allClasses):\n","  for i in range(0, 10):\n","    print(\"Fold value = \", i+1)\n","    print(\"-\"*40)\n","    #call k fold\n","    X_train, X_test, y_train, y_test = kfold(allFeatures, allClasses, i)\n","    #==========================    DEAP GLOBAL VARIABLES (viewable by SCOOP)      ======================\n","    # Create Individual\n","    creator.create(\"FitnessMax\", base.Fitness, weights = (1.0,))\n","    creator.create(\"Individual\", list, fitness = creator.FitnessMax)\n","\n","    # Create Toolbox\n","    toolbox = base.Toolbox()\n","    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n","    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, len(allFeatures.columns) - 1)\n","    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n","\n","    # Continue filling toolbox...\n","    toolbox.register(\"evaluate\", getFitness, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test)\n","    toolbox.register(\"mate\", tools.cxOnePoint)\n","    toolbox.register(\"mutate\", tools.mutFlipBit, indpb = 0.05)\n","    toolbox.register(\"select\", tools.selTournament, tournsize = 7)\n","    #===================================================================================================\n","    \n","    # First, we will apply logistic regression using all the features to acquire a baseline accuracy.\n","    individual = [1 for i in range(len(allFeatures.columns))]\n","    Accuracy = getFitness_all(individual, X_train, X_test, y_train, y_test)\n","    if(Accuracy[0]==100):\n","      continue\n","    #print('\\nAccuracy with all features from SVM: \\t', Accuracy)\n","    # Now, we will apply a genetic algorithm to choose a subset of features that gives a better accuracy than the baseline.\n","    hof,log = getHof(toolbox)\n","    testAccuracyList, validationAccuracyList, individualList, percentileList = getMetrics(hof, X_train, X_test, y_train, y_test)\n","    # Get a list of subsets that performed best on validation data\n","    maxValAccSubsetIndicies = [index for index in range(len(validationAccuracyList)) if validationAccuracyList[index] == max(validationAccuracyList)]\n","    maxValIndividuals = [individualList[index] for index in maxValAccSubsetIndicies]\n","    maxValSubsets = [[list(allFeatures)[index] for index in range(len(individual)) if individual[index] == 1] for individual in maxValIndividuals]\n","    acc_list, predict_list = [], []\n","    print('Optimal Feature Subset(s):')\n","    for i in range(len(maxValAccSubsetIndicies)):\n","        x_train = X_train[maxValSubsets[i]]\n","        x_test = X_test[maxValSubsets[i]]\n","        try:\n","          clf = svm.SVC(kernel = 'poly').fit(x_train, y_train)\n","          y_pred_val = clf.predict(x_test)\n","          acc_list.append(accuracy_score(y_test, y_pred_val)*100)\n","          predict_list.append(y_pred_val)\n","        except:\n","          pass\n","          # store the accuracy, precission and recall in a list then for each fold\n","    # convert list to array\n","    acc_list = np.array(acc_list)\n","    # find the maximum accuracy\n","    result = np.where(acc_list == np.amax(acc_list))[0] # now result has all the highest accuracy index\n","    # check for the precission\n","    temp_acc = acc_list[result[0]]\n","    print(\"Accurcay after fs: \", temp_acc)\n","    draftlist = np.array(predict_list[result[0]])\n","    print(classification_report(y_test, draftlist))\n","    print(\"-\"*40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKwW930Vd8X2"},"outputs":[],"source":["for i in range(27, 32):\n","  print('-'*100)\n","  print(\"Subject No: \", subject_names[i])\n","  print('-'*100)\n","  allFeatures, allClasses = drive(subject_names[i], subject_all_channels[i])\n","  main_code(allFeatures, allClasses)"]},{"cell_type":"markdown","metadata":{"id":"mKm4r5lmigby"},"source":["# Result Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-yAnwrofmBK"},"outputs":[],"source":["[79.6, 61.1, 80.5, 67.8, 66.2, 71.7, 67.6, 68.4, 62.5, 79.2, 48.7, 91.2, 76.3, 72, 75.4, 79.8, 63.1, 67.1, 76.2, 66.3, 62.3, 62.4, 82.8, 49.3, 81.6, 89.4, 60.6, 77.7, 71.3, 74.6, 65.1, 75.6]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZNTQKVXfmHr"},"outputs":[],"source":["[80, 62.5, 77.5, 72.5, 62.5, 72.5, 65, 70, 65, 80, 53.3, 92.5, 77.5, 77.5, 75, 80, 65, 74.2, 76.7, 70, 65.8, 60, 85, 55, 83.3, 90, 68.3, 77.5, 75, 75, 67.5, 75]"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6n5nRt-h94OI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640703395867,"user_tz":-330,"elapsed":657,"user":{"displayName":"SHYAM MARJIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgXWgt6yPRlb1Vc2PDp-7CmKEzAlS0XLO2cxEV=s64","userId":"10874093040693940713"}},"outputId":"07022f66-6857-40e5-878f-d8642b1101eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32,)\n","72.109375\n","9.293591857262454\n"]}],"source":["import numpy as np\n","accuracy = np.array([77.5, 62.5, 77.5, 72.5, 62.5, 72.5, 65, 67.5, 65, 80, 52.5, 92.5, 77.5, 77.5, 75, 80, 57.5, 72.5, 75, \n","                     70, 65, 60, 85, 55, 82.5, 90, 67.5, 77.5, 75, 75, 67.5, 75])\n","print(accuracy.shape)\n","print(np.mean(accuracy))\n","print(np.std(accuracy))"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9Q_jWuId94SW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640703586079,"user_tz":-330,"elapsed":518,"user":{"displayName":"SHYAM MARJIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgXWgt6yPRlb1Vc2PDp-7CmKEzAlS0XLO2cxEV=s64","userId":"10874093040693940713"}},"outputId":"9ec48eb5-0972-41ed-c58b-162bb8c5bdc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32,)\n","74.878125\n","10.337061187028691\n"]}],"source":["pre = np.array([84, 66.6, 89.4, 66.7, 79.5, 76.8, 81, 73.7, 66.3, 82.5, 48.8, 91.4, 79.4, 69.7, 78.7, 84.6, 72.6, \n","                64.1, 83.4, 67.1, 66.9, 70.7, 83.8, 53.6, 84.5, 90.8, 58, 83.3, 70.3, 79.8, 68.2, 79.9])\n","print(pre.shape)\n","print(np.mean(pre))\n","print(np.std(pre))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ezlRhKXR94Ui","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640703760056,"user_tz":-330,"elapsed":545,"user":{"displayName":"SHYAM MARJIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgXWgt6yPRlb1Vc2PDp-7CmKEzAlS0XLO2cxEV=s64","userId":"10874093040693940713"}},"outputId":"e1ffb517-7370-464b-93ae-0a08861a6114"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32,)\n","72.70625\n","8.999963107563275\n"]}],"source":["recall = np.array([80, 62.5, 77.5, 72.5, 62.5, 72.5, 65, 70, 65, 80, 53.3, 92.5, 77.5, 77.5, 75, 80, 65, 74.2, 76.7, 70, 65.8, 60, 85,\n","                   55, 83.3, 90, 68.3, 77.5, 75, 75, 67.5, 75])\n","print(recall.shape)\n","print(np.mean(recall))\n","print(np.std(recall))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lLp8IY8fWATY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640704093877,"user_tz":-330,"elapsed":413,"user":{"displayName":"SHYAM MARJIT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgXWgt6yPRlb1Vc2PDp-7CmKEzAlS0XLO2cxEV=s64","userId":"10874093040693940713"}},"outputId":"8ebfda47-46f0-4389-d2c4-20a6ad6ac8e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(32,)\n","71.04374999999999\n","9.753971546887966\n"]}],"source":["f1 = np.array([79.6, 61.1, 80.5, 67.8, 66.2, 71.7, 67.6, 68.4, 62.5, 79.2, 48.7, 91.2, 76.3, 72, 75.4, 79.8, 63.1, 67.1, 76.2, \n","               66.3, 62.3, 62.4, 82.8, 49.3, 81.6, 89.4, 60.6, 77.7, 71.3, 74.6, 65.1, 75.6])\n","print(f1.shape)\n","print(np.mean(f1))\n","print(np.std(f1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqPa-bpHB6F6"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"STEP: 4C--GA based feature selection from optimal channels + SVM classifier for Four class.ipynb","provenance":[{"file_id":"1lLWkloBVb_CfcrQ_G-6uW8zgLTKbOc7E","timestamp":1639627527651},{"file_id":"1HYesvxoVn-s8CHSqqMROc-FaB65YAOUe","timestamp":1639586905172},{"file_id":"1HLyqqmh4klsQ8H-0e4Eq499vAupq5dS0","timestamp":1639306093118},{"file_id":"1KXk5nWjSgHzZ6ccp66jHRDZI4bJMslHn","timestamp":1639196338642},{"file_id":"1yUzEFwdmCNH_qb7NPCgLlYElOhT12erL","timestamp":1634187566670},{"file_id":"1SPeDVHCHi4uFuamkolvaep7nGU7mOyiv","timestamp":1634152149383},{"file_id":"1aD6aXmoab59Jb7cxRTbtGKDfdiG7ianV","timestamp":1634151736936},{"file_id":"1luLhv4lQ-SruSeQqeX8bZnYPR-_TYxDk","timestamp":1634111894346},{"file_id":"11OHLRm15JjC9C-Y-R_LGBacQkwDFduQh","timestamp":1633972949871},{"file_id":"1qrk7-0VYo258YnzRzG4ztcbrT3zWq-HG","timestamp":1633964775809}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}